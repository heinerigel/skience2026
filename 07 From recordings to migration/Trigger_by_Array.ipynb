{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7cc10042-6134-4894-939f-347b05a9358e",
   "metadata": {},
   "source": [
    "# The Use of Arrays on Glaciers\n",
    "   <img src=\"pictures/Grenzgletscher_oben.jpg\" alt=\"Grenzgletscher 24\" width=\"600\"/>  <img src=\"pictures/Upper_3700.jpg\" alt=\"Grenzgletscher 3700m\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78dd7187-4c32-45d1-ac1c-52bead169c8e",
   "metadata": {},
   "source": [
    "<b> Developed by J. Wassermann modified by N. Richels </b> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3322270e-97dd-4e14-a7d8-322d3c36a44d",
   "metadata": {},
   "source": [
    "In this pretty dense notebook, we try to do better in the sense \n",
    "that we want also to obtain features which are seperating different types of events\n",
    "If you know nothing about the signals, their characteristics and also their \n",
    "locations its often prefferable to start with arrays of seismic sensors. <br>\n",
    "A seismic array <b>(@Heiner: yes, also 6C would be possible)</b> allows you to decompose \n",
    "the wavefield into its components and get first insights in the velocity structure \n",
    "as well as location of the corresponding events. In case of tremor like signals arrays \n",
    "are often the only possibility to estimate the source location and possible change of \n",
    "the source location."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "048feac3-4aaa-4d67-95eb-a5bcff6e1a35",
   "metadata": {},
   "source": [
    "There are various arraytools on the market - also a standard module in obspy, we use for your (and my) convenience a differrent module which is the result of\n",
    "a <br> <b>Skience Workshop in 2014(!)</b>. <br> You might visit the tutorial as well (be aware still a lot of code polishing needed!) <br> http://github.com/jwassermann/obspy_arraytools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5920ec05-1b03-4480-aea6-720fc189bcb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "from obspy import Stream,Trace,UTCDateTime,read_inventory \n",
    "from obspy.core.inventory.inventory import Inventory\n",
    "from obspy.core import AttribDict\n",
    "\n",
    "import matplotlib.dates as mdates\n",
    "from matplotlib.dates import julian2num\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "\n",
    "from obspy.clients.filesystem import sds\n",
    "from obspy.signal.invsim import cosine_taper\n",
    "\n",
    "import obspy_arraytools as AA\n",
    "\n",
    "SDS_DATA_PATH = \"/Users/jowa/Skience2026_Data/data_sds\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e541eb8-ba67-458a-b00c-5bf426775521",
   "metadata": {},
   "outputs": [],
   "source": [
    "sds_rootdir = SDS_DATA_PATH\n",
    "arraystats = [\"XG.UP1..GLZ\",\"XG.UP2..GLZ\",\"XG.UP3..GLZ\",\"XG.UP4..GLZ\",\"XG.UP5..GLZ\",\"XG.UP6..GLZ\"]\n",
    "output_path = \"./Grenzgletscher_fk\"\n",
    "figure_path = \"./Grenzgletscher_fk/figure\"\n",
    "fl=10.0 #lower frequency \n",
    "fh=80.00 #upper frequency limit\n",
    "win_len=2.0  #windowlenth of sliding window in seconds\n",
    "win_frac=0.1 #step width of sliding window\n",
    "sll_x=-0.5 #lower bound of slowness s/km\n",
    "slm_x=0.5 #upper bound of slowness s/km\n",
    "sll_y=-0.5 #ditto but y axis\n",
    "slm_y=0.5 #ditto\n",
    "sl_s=0.025 #grid with for search area\n",
    "thres_rel = 0.6 #semblance value for trigger\n",
    "\n",
    "client = sds.Client(sds_root=sds_rootdir)\n",
    "#Let's analyse just two hours of data\n",
    "ts= t = UTCDateTime(\"2024-03-20T00:00:00\")\n",
    "e = UTCDateTime(\"2024-03-20T02:00:00\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5654b3f2-1449-47f1-8116-32816d51390e",
   "metadata": {},
   "source": [
    "## Note\n",
    "#### we first create outputs from the array analysis on which we later trigger\n",
    "#### The following cell is creating a new sds directory an appends hourly segments to existing files\n",
    "#### If you re-run this cell you have to <b> delete </b>the corresponding folder, otherwise you double your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c2a640-286f-4c47-bf20-56e47887ecc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check if you realy want doing this\n",
    "!rm -rf \"%s/%04d\"%(out_path,ts.year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7539f52-7c11-4e48-8066-cf2cf574f005",
   "metadata": {},
   "outputs": [],
   "source": [
    "while (ts+3600) < e:\n",
    "    start = (ts)\n",
    "    end = (ts+3600)\n",
    "    ts += 3600\n",
    "    try:\n",
    "        sz = Stream()\n",
    "        inv= Inventory()\n",
    "        i = 0\n",
    "        #reading in data and metadata of the array(s)\n",
    "        for station in arraystats:\n",
    "            net,stat,loc,chan=station.split('.')\n",
    "            tr = client.get_waveforms(network=net,station=stat,location=loc,channel=chan, starttime=start, endtime=end)\n",
    "            ii = read_inventory(\"./stationxml/station_%s_%s.xml\"%(net,stat))\n",
    "            tr.merge()\n",
    "            if tr[0].stats.endtime < end or tr[0].stats.starttime > start:\n",
    "                print(\"trace \",tr, \"too short\")\n",
    "            else: \n",
    "                print(tr)\n",
    "                sz += tr\n",
    "                inv += ii\n",
    "        sz.merge()\n",
    "        sz.detrend(\"linear\")\n",
    "        sz.attach_response(inv)\n",
    "        \n",
    "        #we restrict ourself to the vertical components.... at least for now\n",
    "        vc = sz.select(component=\"Z\")\n",
    "        \n",
    "        #veeeery convenient function \n",
    "        # handles all geometry issues of the array\n",
    "        array = AA.SeismicArray(\"\",inv)\n",
    "        array.inventory_cull(vc)\n",
    "        \n",
    "        print(\"Center of gravity: \",array.center_of_gravity)\n",
    "        \n",
    "        outray = 0. \n",
    "        \n",
    "        #we use here a covariance based FK-analysis\n",
    "        outray = array.fk_analysis(vc, frqlow=fl, frqhigh=fh, prefilter=True,\\\n",
    "                         static3d=False, array_response=False,vel_corr=4.8, wlen=win_len,\\\n",
    "                         wfrac=win_frac,sec_km=True,\n",
    "                         slx=(sll_x,slm_x),sly=(sll_y,slm_y),\n",
    "                         sls=sl_s)\n",
    "\n",
    "        trace1 = Trace(data=outray.max_rel_power)\n",
    "        trace1.stats.channel = 'REL'\n",
    "        out = outray.max_rel_power\n",
    "        trace2 = Trace(data=outray.max_abs_power)\n",
    "        trace2.stats.channel = 'ABS'\n",
    "        out = np.vstack([out,outray.max_abs_power])\n",
    "\n",
    "        trace3 = Trace(data=outray.max_pow_baz)\n",
    "        trace3.stats.channel = 'BACK'\n",
    "        out = np.vstack([out,outray.max_pow_baz])\n",
    "\n",
    "        trace4 = Trace(data=outray.max_pow_slow)\n",
    "        trace4.stats.channel = 'SLOW'\n",
    "        out = np.vstack([out,outray.max_pow_slow])\n",
    "\n",
    "        #saving f-k analysis results into mseed file\n",
    "        fk = Stream()\n",
    "        tr = Trace()\n",
    "\n",
    "        delta = outray.timestep\n",
    "\n",
    "        tr.stats.network = outray.inventory.networks[0].code\n",
    "        tr.stats.station = outray.inventory.networks[0][0].code\n",
    "        tr.stats.channel = \"ZGC\"\n",
    "        tr.stats.location = \"\"\n",
    "        tr.data = outray.max_rel_power\n",
    "        tr.stats.starttime = outray.starttime\n",
    "        tr.stats.delta = delta\n",
    "\n",
    "        fk += tr\n",
    "\n",
    "        tr = Trace()\n",
    "        tr.stats.network = outray.inventory.networks[0].code\n",
    "        tr.stats.station = outray.inventory.networks[0][0].code\n",
    "        tr.stats.channel = \"ZGI\"\n",
    "        tr.stats.location = \"\"\n",
    "        tr.stats.starttime = outray.starttime\n",
    "        tr.data = outray.max_abs_power\n",
    "        tr.stats.delta = delta\n",
    "\n",
    "        fk += tr\n",
    "        tr = Trace()\n",
    "        tr.stats.network = outray.inventory.networks[0].code\n",
    "        tr.stats.station = outray.inventory.networks[0][0].code\n",
    "        tr.stats.channel = \"ZGS\"\n",
    "        tr.stats.location = \"\"\n",
    "        tr.stats.starttime = outray.starttime\n",
    "        tr.data = outray.max_pow_baz\n",
    "        tr.stats.delta = delta\n",
    "\n",
    "        fk += tr\n",
    "\n",
    "        tr = Trace()\n",
    "        tr.stats.network = outray.inventory.networks[0].code\n",
    "        tr.stats.station = outray.inventory.networks[0][0].code\n",
    "        tr.stats.channel = \"ZGA\"\n",
    "        tr.stats.location = \"\"\n",
    "        tr.stats.starttime = outray.starttime\n",
    "        tr.data = outray.max_pow_slow\n",
    "        tr.stats.delta = delta\n",
    "\n",
    "        fk += tr\n",
    "\n",
    "        myday = \"%03d\"%fk[0].stats.starttime.julday\n",
    "\n",
    "        pathyear = str(fk[0].stats.starttime.year)\n",
    "        # open catalog file in read and write mode in case we are continuing d/l,\n",
    "        # so we can append to the file\n",
    "        mydatapath = os.path.join(output_path, pathyear)\n",
    "        # create datapath \n",
    "        if not os.path.exists(mydatapath):\n",
    "            os.mkdir(mydatapath)\n",
    "\n",
    "        mydatapath = os.path.join(mydatapath, fk[0].stats.network)\n",
    "        if not os.path.exists(mydatapath):\n",
    "            os.mkdir(mydatapath)\n",
    "\n",
    "        mydatapath = os.path.join(mydatapath, fk[0].stats.station)\n",
    "\n",
    "        # create datapath \n",
    "        if not os.path.exists(mydatapath):\n",
    "                os.mkdir(mydatapath)\n",
    "\n",
    "\n",
    "        for tr in fk:\n",
    "            print(\"saving to \" + mydatapath)\n",
    "            print(tr)\n",
    "            mydatapathchannel = os.path.join(mydatapath,tr.stats.channel + \".D\")\n",
    "\n",
    "            if not os.path.exists(mydatapathchannel):\n",
    "                os.mkdir(mydatapathchannel)\n",
    "\n",
    "            netFile = tr.stats.network + \".\" + tr.stats.station +  \".\" + tr.stats.location + \".\" + tr.stats.channel+ \".D.\" + pathyear + \".\" + myday\n",
    "            netFileout = os.path.join(mydatapathchannel, netFile)\n",
    "\n",
    "            # try to open File\n",
    "            print(netFileout)\n",
    "            try:\n",
    "                netFileout = open(netFileout, 'ab')\n",
    "            except:\n",
    "                netFileout = open(netFileout, 'w')\n",
    "            tr.write(netFileout , format='MSEED',encoding=\"FLOAT64\")\n",
    "            netFileout.close()\n",
    "\n",
    "        #print(outray)\n",
    "        # Plot FK\n",
    "        labels = ['ref','rel.power', 'abs.power', 'baz', 'slow']\n",
    "        xlocator = mdates.AutoDateLocator()\n",
    "        fig = plt.figure()\n",
    "        alphas = out[0,:]\n",
    "        condition1 = (out[0,:] < thres_rel)\n",
    "        condition2 = (out[3,:] > 0.4) \n",
    "        tt = np.ma.masked_array(fk[0].times(\"matplotlib\"),mask=condition1)\n",
    "        tt = np.ma.masked_array(tt,mask=condition2)\n",
    "        axis = []\n",
    "\n",
    "        for i, lab in enumerate(labels):\n",
    "            try:\n",
    "                if i == 0:\n",
    "                    ax = fig.add_subplot(5, 1, i + 1,sharex=None)\n",
    "                    ax.plot(vc[0].times(\"matplotlib\"),vc[0].data)\n",
    "                else:\n",
    "                    ax = fig.add_subplot(5, 1, i + 1,sharex=axis[0])\n",
    "                    mask_v = np.ma.masked_array(out[i-1,:],mask=condition1)\n",
    "                    mask_v = np.ma.masked_array(mask_v,mask=condition2)\n",
    "                    ax.scatter(tt,mask_v, c=out[0,:], alpha=alphas,\n",
    "                       edgecolors='none', cmap=cm.viridis_r)\n",
    "                    ax.set_ylabel(lab)\n",
    "                    ax.set_ylim(mask_v.min()-0.1, mask_v.max()+0.1)\n",
    "                    ax.xaxis.set_major_locator(xlocator)\n",
    "                    ax.xaxis.set_major_formatter(mdates.AutoDateFormatter(xlocator))\n",
    "                axis.append(ax)\n",
    "            except Exception as er:\n",
    "                sys.stderr.write(\"Error:\" + str(er))\n",
    "                traceback.print_exc()\n",
    "        fig.suptitle( 'jane-fk %s' % ( start ))\n",
    "        fig.autofmt_xdate()\n",
    "        fig.subplots_adjust(left=0.15, top=0.95, right=0.95, bottom=0.2, hspace=0)\n",
    "        plt.savefig(\"%s/FK-%s.png\"%(figure_path,start.strftime('%Y-%m-%dT%H')))\n",
    "        #plt.show()\n",
    "        plt.close(\"all\")\n",
    "    except:\n",
    "        print(start)\n",
    "        continue\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2dd24bc-1ec7-4986-bcb0-53c5665599ea",
   "metadata": {},
   "source": [
    "# Now we trigger ...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a96c00-47b4-4fd0-9cdd-e5b08c938fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from obspy.core import UTCDateTime, Stream, AttribDict\n",
    "from obspy import read_inventory\n",
    "from obspy.clients.filesystem import sds\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from matplotlib.colorbar import ColorbarBase\n",
    "from matplotlib.colors import Normalize\n",
    "import numpy as np\n",
    "import logging\n",
    "import os\n",
    "from datetime import timedelta\n",
    "import matplotlib as mpl\n",
    "from matplotlib.gridspec import GridSpec\n",
    "\n",
    "par = AttribDict()\n",
    "par.filter = AttribDict(type=\"bandpass\", freqmin=5.0, freqmax=30.0,\n",
    "                            corners=2, zerophase=True)\n",
    "\n",
    "# Set better default style for matplotlib\n",
    "plt.style.use('seaborn-v0_8')\n",
    "mpl.rcParams['axes.facecolor'] = 'white'\n",
    "mpl.rcParams['figure.facecolor'] = 'white'\n",
    "mpl.rcParams['font.family'] = 'sans-serif'\n",
    "\n",
    "# Add logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "out_file = \"./Grenzgletscher_fk/fk_trigger.csv\"\n",
    "sds_rootdir = \"./Grenzgletscher_fk/\"\n",
    "save_path = \"./Grenzgletscher_fk/trig_figs/\"\n",
    "data_dir = SDS_DATA_PATH\n",
    "\n",
    "# Create directory if it doesn't exist\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "cl = sds.Client(sds_root=sds_rootdir)\n",
    "clw = sds.Client(sds_root=data_dir)\n",
    "\n",
    "ostart = start = UTCDateTime(2024, 3, 20, 00)\n",
    "end = UTCDateTime(2024, 3, 21)\n",
    "trig_times = []\n",
    "\n",
    "# Debug info\n",
    "logger.info(f\"Starting trigger detection from {ostart} to {end}\")\n",
    "trigger_count = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5abd91ce-2fa7-4ba2-883e-9819e374913b",
   "metadata": {},
   "source": [
    "<br>\n",
    "For triggering we simply use the semblance values (i.e. multi-trace coherence) and slowness (i.e. body waves) <br>\n",
    "as measure of an indication of an incoming event\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad611796-c3fc-407a-bfcd-0a7c879c6be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Thresholds for detection\n",
    "relp_threshold = 0.6\n",
    "slow_threshold = 0.5\n",
    "min_trigger_separation = 2  # Minimum seconds between triggers\n",
    "\n",
    "last_trigger_abs_time = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cbc8da4-f6d2-426c-914e-f36b9982b92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "while start + 1*3600 < end:\n",
    "    endy = start + 1*3600\n",
    "    try:\n",
    "        logger.info(f\"Processing window: {start} - {endy}\")\n",
    "        st = cl.get_waveforms(network=\"XG\", station=\"UP1\", location=\"\", channel=\"ZG?\", starttime=start, endtime=endy)\n",
    "        st.merge()\n",
    "        \n",
    "        # Check for required channels\n",
    "        channels = [tr.stats.channel for tr in st]\n",
    "        if \"ZGC\" not in channels or \"ZGA\" not in channels:\n",
    "            logger.warning(f\"Missing required channels. Available: {channels}\")\n",
    "            start += 1*3600\n",
    "            continue\n",
    "        \n",
    "        relp = st.select(channel=\"ZGC\")[0]\n",
    "        slow = st.select(channel=\"ZGA\")[0]\n",
    "        \n",
    "        window_triggers = 0\n",
    "        \n",
    "        for i in range(1, relp.stats.npts):\n",
    "            # Check if conditions are met\n",
    "            if relp.data[i] > relp_threshold and slow.data[i] < slow_threshold:\n",
    "                # Check for state transition\n",
    "                if i > 5 and (\n",
    "                    (np.mean(relp.data[i-1:i]) < relp_threshold) or \n",
    "                    (np.mean(slow.data[i-1:i]) > slow_threshold)\n",
    "                ):\n",
    "                    trig_time = relp.times(reftime=ostart)[i]\n",
    "                    abs_time = ostart + trig_time\n",
    "                    \n",
    "                    # Check minimum separation\n",
    "                    if last_trigger_abs_time is None or (abs_time - last_trigger_abs_time) > min_trigger_separation:\n",
    "                        trig_times.append(trig_time)\n",
    "                        logger.info(f\"Trigger detected at index {i}: relative time={trig_time}, absolute time={abs_time}\")\n",
    "                        window_triggers += 1\n",
    "                        last_trigger_abs_time = abs_time\n",
    "                    else:\n",
    "                        logger.info(f\"Skipping close trigger at {abs_time} (too close to previous)\")\n",
    "        logger.info(f\"Found {window_triggers} triggers in this window\")\n",
    "        trigger_count += window_triggers\n",
    "        start += 1*3600\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing window {start}-{endy}: {e}\")\n",
    "        start += 1*3600\n",
    "        continue\n",
    "\n",
    "logger.info(f\"Total triggers detected: {trigger_count}\")\n",
    "logger.info(f\"Writing triggers to {out_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19501822-53a9-42f9-a5af-c01e4999b084",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add validation before writing to CSV\n",
    "valid_triggers = []\n",
    "for j in trig_times:\n",
    "    trigger_time = ostart + j\n",
    "    valid_triggers.append(trigger_time)\n",
    "\n",
    "# Write triggers to file\n",
    "with open(out_file, \"w\") as fo:\n",
    "    for trigger_time in valid_triggers:\n",
    "        fo.write(\"%s\\n\" % trigger_time)\n",
    "\n",
    "logger.info(f\"Written {len(valid_triggers)} triggers to CSV file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437c2612-e759-4ead-9558-aeab7ca248fa",
   "metadata": {},
   "source": [
    "#### Now extensive plotting is done ... We will see later why"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5107194b-26f7-4440-be98-38fe0144cab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process individual events\n",
    "all_baz = []\n",
    "all_slow = []\n",
    "all_rp = []\n",
    "\n",
    "# Define the global colormap\n",
    "global_cmap = plt.cm.viridis\n",
    "\n",
    "# Define wave velocities for incidence angle calculation\n",
    "p_velocity = 3.8  # km/s for P-waves\n",
    "s_velocity = 1.8  # km/s for S-waves\n",
    "\n",
    "logger.info(f\"Processing {len(trig_times)} individual events\")\n",
    "\n",
    "#Track successful plot creation\n",
    "successful_plots = 0\n",
    "failed_plots = 0\n",
    "for j_idx, j in enumerate(trig_times):\n",
    "    try:\n",
    "        event_time = ostart + j\n",
    "        logger.info(f\"Processing event {j_idx+1}/{len(trig_times)} at {event_time}\")\n",
    "        \n",
    "        # Get waveform data with a window around the event time\n",
    "        try:\n",
    "            st = clw.get_waveforms(network=\"XG\", station=\"UP1\", location=\"\", channel=\"??Z\", \n",
    "                                   starttime=(event_time-1), endtime=event_time+10)\n",
    "            ar = cl.get_waveforms(network=\"XG\", station=\"UP1\", location=\"\", channel=\"ZG?\", \n",
    "                                  starttime=(event_time-1), endtime=event_time+10)\n",
    "        except Exception as data_error:\n",
    "            logger.error(f\"Failed to get waveform data for event at {event_time}: {data_error}\")\n",
    "            failed_plots += 1\n",
    "            continue\n",
    "        \n",
    "        # Check if valid data\n",
    "        if len(st) == 0 or len(ar) == 0:\n",
    "            logger.warning(f\"No data found for event at {event_time}, skipping\")\n",
    "            failed_plots += 1\n",
    "            continue\n",
    "            \n",
    "        # Check for required channels\n",
    "        ar_channels = [tr.stats.channel for tr in ar]\n",
    "        if \"ZGC\" not in ar_channels or \"ZGA\" not in ar_channels or \"ZGS\" not in ar_channels:\n",
    "            logger.warning(f\"Missing required array channels for event at {event_time}, skipping. Available: {ar_channels}\")\n",
    "            failed_plots += 1\n",
    "            continue\n",
    "            \n",
    "        # Check if there's a valid vertical component\n",
    "        if not st.select(component=\"Z\"):\n",
    "            logger.warning(f\"No vertical component found for event at {event_time}, skipping\")\n",
    "            failed_plots += 1\n",
    "            continue\n",
    "            \n",
    "        # Process waveforms\n",
    "        st.detrend(\"linear\")\n",
    "        st.taper(type='cosine', max_percentage=0.05)\n",
    "        st.filter(**par.filter)\n",
    "        # Extract data\n",
    "        if ar.select(channel=\"ZGC\"):\n",
    "            rel_power = ar.select(channel=\"ZGC\")[0].data\n",
    "            all_rp.append(rel_power)\n",
    "        else:\n",
    "            logger.warning(f\"Missing ZGC channel for event at {event_time}\")\n",
    "            failed_plots += 1\n",
    "            continue\n",
    "            \n",
    "        if ar.select(channel=\"ZGS\"):\n",
    "            baz = ar.select(channel=\"ZGS\")[0].data\n",
    "            all_baz.append(baz)\n",
    "        else:\n",
    "            logger.warning(f\"Missing ZGS channel for event at {event_time}\")\n",
    "            failed_plots += 1\n",
    "            continue\n",
    "            \n",
    "        if ar.select(channel=\"ZGA\"):\n",
    "            slow = ar.select(channel=\"ZGA\")[0].data\n",
    "            all_slow.append(slow)\n",
    "        else:\n",
    "            logger.warning(f\"Missing ZGA channel for event at {event_time}\")\n",
    "            failed_plots += 1\n",
    "            continue\n",
    "            \n",
    "        # Calculate incidence angles based on slowness values\n",
    "        incidence_angles = []\n",
    "        wave_types = []\n",
    "        for s in slow:\n",
    "            if s < 0.3:  # P-wave region\n",
    "                sin_i = min(p_velocity * s, 0.99)\n",
    "                angle = np.degrees(np.arcsin(sin_i))\n",
    "                wave_type = \"P\"\n",
    "            elif s <= 0.6:  # S-wave region\n",
    "                if s_velocity * s > 0.99:\n",
    "                    # Scale between 60-85 degrees based on the slowness value\n",
    "                    angle = 60 + 25 * (s - 0.2) / 0.3\n",
    "                else:\n",
    "                    sin_i = min(s_velocity * s, 0.99)\n",
    "                    angle = np.degrees(np.arcsin(sin_i))\n",
    "                wave_type = \"S\"\n",
    "            else:\n",
    "                # For values outside velocity model assumptions\n",
    "                angle = np.nan\n",
    "                wave_type = \"Unknown\"\n",
    "                \n",
    "            incidence_angles.append(angle)\n",
    "            wave_types.append(wave_type)\n",
    "        \n",
    "        # Convert to numpy arrays\n",
    "        incidence_angles = np.array(incidence_angles)\n",
    "        wave_types = np.array(wave_types)\n",
    "        \n",
    "        # Close any existing figures\n",
    "        plt.close('all')\n",
    "        \n",
    "        # Create figure\n",
    "        fig = plt.figure(figsize=(12, 16), dpi=100)\n",
    "        fig.suptitle(f\"Seismic Event Analysis - {event_time.strftime('%Y-%m-%d %H:%M:%S')}\", \n",
    "                    fontsize=16, fontweight='bold', y=0.98)\n",
    "        \n",
    "        # Create a grid layout with 3 rows and 2 columns\n",
    "        gs = plt.GridSpec(3, 2, figure=fig, height_ratios=[1, 1, 1], width_ratios=[1, 1],\n",
    "                          hspace=0.35, wspace=0.35)\n",
    "        common_time_limits = None\n",
    "        if st.select(component=\"Z\"):\n",
    "            vert_tr = st.select(component=\"Z\")[0]\n",
    "            time_data = vert_tr.times(\"matplotlib\")\n",
    "            \n",
    "            # Get time limits for alignment\n",
    "            end_time = max(time_data)\n",
    "            start_time = min(time_data)\n",
    "            \n",
    "            # Common time limits for all time-based plots\n",
    "            common_time_limits = [start_time, end_time]\n",
    "            \n",
    "            # Create the time locator for all plots\n",
    "            seconds_locator = mdates.SecondLocator(interval=1)\n",
    "            seconds_formatter = mdates.DateFormatter('%H:%M:%S')\n",
    "        else:\n",
    "            logger.warning(\"No vertical component data for establishing time limits\")\n",
    "            failed_plots += 1\n",
    "            continue\n",
    "        \n",
    "        # PLOT 1: Seismogram - Row 1, Col 1 (Upper Left)\n",
    "        axtrace = fig.add_subplot(gs[0, 0])\n",
    "        # Plot the vertical component data\n",
    "        if st.select(component=\"Z\"):\n",
    "            axtrace.plot(time_data, vert_tr.data, 'k', linewidth=1.2)\n",
    "            axtrace.ticklabel_format(axis='y', style='sci', scilimits=(-2,2))\n",
    "            axtrace.set_ylabel('Amplitude [mm/s]', color='k', fontsize=11, fontweight='bold')\n",
    "            axtrace.set_title('Vertical Component Waveform', fontsize=12, fontweight='bold', pad=10)\n",
    "            \n",
    "            # Set x-axis ticks every second\n",
    "            axtrace.xaxis.set_major_locator(seconds_locator)\n",
    "            axtrace.xaxis.set_major_formatter(seconds_formatter)\n",
    "            \n",
    "            # Set time limits\n",
    "            axtrace.set_xlim(common_time_limits)\n",
    "            \n",
    "            # Enhance grid for seismogram\n",
    "            axtrace.grid(True, which='both', axis='x', color='gray', alpha=0.5, linestyle='-')\n",
    "            axtrace.grid(True, which='major', axis='y', color='gray', alpha=0.5, linestyle='-')\n",
    "            \n",
    "            # Rotate time labels\n",
    "            plt.setp(axtrace.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "        else:\n",
    "            logger.warning(\"No vertical component data for trace plot\")\n",
    "            failed_plots += 1\n",
    "            continue\n",
    "            \n",
    "        # PLOT 2: Polar Plot - Row 1, Col 2 (Upper Right)\n",
    "        polar_ax = fig.add_subplot(gs[0, 1], projection='polar')\n",
    "        # Check for the necessary data\n",
    "        if len(baz) > 0 and len(slow) > 0 and len(rel_power) > 0:\n",
    "            # Convert backazimuth to radians\n",
    "            baz_rad = np.radians(baz)\n",
    "            baz_rad[baz_rad < 0] += 2*np.pi\n",
    "            baz_rad[baz_rad > 2*np.pi] -= 2*np.pi\n",
    "            \n",
    "            # Create 2D histogram for polar plot\n",
    "            N = int(360./5.)  # 5-degree bins\n",
    "            abins = np.arange(N + 1) * 2*np.pi / N\n",
    "            sbins = np.linspace(0, 0.4, 20) \n",
    "            \n",
    "            hist, baz_edges, sl_edges = np.histogram2d(baz_rad, slow, bins=[abins, sbins], weights=rel_power)\n",
    "            \n",
    "            # Create meshgrid for pcolormesh\n",
    "            A, S = np.meshgrid(abins, sbins)\n",
    "\n",
    "            polar_ax.set_theta_zero_location(\"N\")\n",
    "            polar_ax.set_theta_direction(-1)\n",
    "            \n",
    "            # Use pcolormesh for polar plot with the global colormap\n",
    "            pcm = polar_ax.pcolormesh(A, S, hist.T, cmap=global_cmap, alpha=0.7, shading='auto')\n",
    "            \n",
    "            # Improve polar plot settings\n",
    "            polar_ax.grid(True, linewidth=1.5)\n",
    "            \n",
    "            # Add radial labels\n",
    "            polar_ax.set_rticks([0.1, 0.2, 0.3, 0.4])\n",
    "            polar_ax.set_rlabel_position(135)\n",
    "            polar_ax.set_rmax(0.4)\n",
    "            polar_ax.set_title('Polar Plot: Backazimuth vs. Slowness', fontsize=12, fontweight='bold', pad=15)\n",
    "        else:\n",
    "            logger.warning(\"Missing data for polar plot\")\n",
    "            \n",
    "        # PLOT 3: Spectrogram - Row 2, Col 1 (Middle Left)\n",
    "        axspec = fig.add_subplot(gs[1, 0])\n",
    "        # Get the vertical component data for spectrogram\n",
    "        if st.select(component=\"Z\"):\n",
    "            tr = st.select(component=\"Z\")[0]\n",
    "    \n",
    "            try:\n",
    "                # Calculate spectrogram\n",
    "                specgram = tr.spectrogram(wlen=0.5, per_lap=0.9, show=False, axes=axspec)\n",
    "        \n",
    "                # Limit frequency range\n",
    "                axspec.set_ylim(1, 25)  # Limit frequency to 1-25 Hz\n",
    "        \n",
    "                # Set labels and grid\n",
    "                axspec.set_ylabel('Frequency [Hz]', fontsize=11, fontweight='bold')\n",
    "                \n",
    "                # Clear the current x-axis labels and ticks\n",
    "                axspec.set_xticklabels([])\n",
    "                axspec.set_xticks([])\n",
    "                \n",
    "                # Create secondary axis that matches seismogram time\n",
    "                ax2 = axspec.twiny()\n",
    "                ax2.set_xlim(common_time_limits)\n",
    "                ax2.xaxis.set_major_locator(seconds_locator)\n",
    "                ax2.xaxis.set_major_formatter(seconds_formatter)\n",
    "                ax2.xaxis.tick_bottom()\n",
    "                ax2.xaxis.set_label_position('bottom')\n",
    "                ax2.tick_params(axis='x', pad=10)\n",
    "                \n",
    "                # Rotate time labels\n",
    "                plt.setp(ax2.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "                \n",
    "                # Add horizontal grid lines\n",
    "                axspec.grid(True, which='major', axis='y', color='gray', alpha=0.01, linestyle='-')\n",
    "                \n",
    "                # Add vertical grid lines\n",
    "                for pos in ax2.get_xticks():\n",
    "                    axspec.axvline(pos, color='gray', alpha=0.01, linestyle='-')\n",
    "                \n",
    "                # Set title above the plot\n",
    "                axspec.set_title('Spectrogram', fontsize=12, fontweight='bold', pad=10)\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error creating spectrogram: {e}\")\n",
    "        else:\n",
    "            logger.warning(\"No vertical component data for spectrogram\")\n",
    "            \n",
    "        # PLOT 4: Incidence Angle - Row 2, Col 2 (Middle Right)\n",
    "        axangle = fig.add_subplot(gs[1, 1])\n",
    "        \n",
    "        # Only plot valid incidence angles\n",
    "        valid_mask = ~np.isnan(incidence_angles)\n",
    "        if any(valid_mask):\n",
    "            # Get matplotlib times for the incidence angle data\n",
    "            angle_times = ar.select(channel=\"ZGA\")[0].times(\"matplotlib\")\n",
    "            \n",
    "            # Calculate point sizes based on relative power if not already defined\n",
    "            rel_power_norm = rel_power / np.max(rel_power) if np.max(rel_power) > 0 else np.zeros_like(rel_power)\n",
    "            sizes = 20 + 100 * rel_power_norm\n",
    "            \n",
    "            # Use the same marker sizing and coloring scheme as the other plots\n",
    "            scatter_angle = axangle.scatter(\n",
    "                angle_times[valid_mask], \n",
    "                incidence_angles[valid_mask],\n",
    "                c=rel_power[valid_mask], \n",
    "                cmap=global_cmap, \n",
    "                s=sizes[valid_mask], \n",
    "                alpha=0.7\n",
    "            )\n",
    "\n",
    "            # Add markers to indicate wave type\n",
    "            p_mask = np.logical_and(valid_mask, np.array(wave_types) == \"P\")\n",
    "            if any(p_mask):\n",
    "                axangle.scatter(\n",
    "                    angle_times[p_mask], \n",
    "                    incidence_angles[p_mask],\n",
    "                    s=30, alpha=0.7, facecolors='none', edgecolors='blue',\n",
    "                    linewidth=1.5, marker='o', label='P-wave'\n",
    "                )\n",
    "                \n",
    "            s_mask = np.logical_and(valid_mask, np.array(wave_types) == \"S\")\n",
    "            if any(s_mask):\n",
    "                axangle.scatter(\n",
    "                    angle_times[s_mask], \n",
    "                    incidence_angles[s_mask],\n",
    "                    s=30, alpha=0.7, facecolors='none', edgecolors='red',\n",
    "                    linewidth=1.5, marker='s', label='S-wave'\n",
    "                )\n",
    "            \n",
    "            axangle.set_ylabel('Incidence Angle [deg]', fontsize=10, fontweight='bold')\n",
    "            axangle.set_title('Incidence Angle vs. Time', fontsize=11, fontweight='bold', pad=10)\n",
    "            \n",
    "            # Set x-axis ticks every second\n",
    "            axangle.xaxis.set_major_locator(seconds_locator)\n",
    "            axangle.xaxis.set_major_formatter(seconds_formatter)\n",
    "            \n",
    "            # Set time limits to match seismogram\n",
    "            axangle.set_xlim(common_time_limits)\n",
    "            \n",
    "            # Set reasonable y-limits for the plot\n",
    "            axangle.set_ylim(0, 90)\n",
    "            \n",
    "            # Enhanced grid with lines every second\n",
    "            axangle.grid(True, which='major', axis='both', color='gray', alpha=0.5, linestyle='-')\n",
    "            axangle.legend(loc='upper right', fontsize=9)\n",
    "\n",
    "            # Rotate time labels\n",
    "            plt.setp(axangle.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "        else:\n",
    "            logger.warning(\"No valid incidence angle data for plot\")\n",
    "        \n",
    "        # PLOT 5: Backazimuth - Row 3, Col 1 (Lower Left)\n",
    "        axbaz = fig.add_subplot(gs[2, 0])\n",
    "        \n",
    "        # Get matplotlib times for the backazimuth data\n",
    "        if ar.select(channel=\"ZGS\") and len(ar.select(channel=\"ZGS\")[0].data) > 0:\n",
    "            baz_times = ar.select(channel=\"ZGS\")[0].times(\"matplotlib\")\n",
    "            \n",
    "            # Check for matching data lengths\n",
    "            if len(baz_times) == len(baz) and len(baz) == len(rel_power):\n",
    "                scatter_baz = axbaz.scatter(baz_times, baz, \n",
    "                           c=rel_power, cmap=global_cmap, s=sizes, alpha=0.7)\n",
    "                axbaz.set_ylabel('Backazimuth [deg]', fontsize=11, fontweight='bold')\n",
    "                axbaz.set_xlabel('Time (UTC)', fontsize=11, fontweight='bold')\n",
    "                axbaz.set_ylim(0, 360)\n",
    "                axbaz.set_yticks([0, 90, 180, 270, 360])\n",
    "                axbaz.set_title('Backazimuth vs. Time', fontsize=12, fontweight='bold', pad=10)\n",
    "                \n",
    "                # Set x-axis ticks every second\n",
    "                axbaz.xaxis.set_major_locator(seconds_locator)\n",
    "                axbaz.xaxis.set_major_formatter(seconds_formatter)\n",
    "                \n",
    "                # Set time limits to match seismogram\n",
    "                axbaz.set_xlim(common_time_limits)\n",
    "                \n",
    "                # Enhanced grid with lines\n",
    "                axbaz.grid(True, which='major', axis='both', color='gray', alpha=0.5, linestyle='-')\n",
    "                \n",
    "                # Rotate time labels\n",
    "                plt.setp(axbaz.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "            else:\n",
    "                logger.warning(f\"Data length mismatch in backazimuth plot\")\n",
    "        else:\n",
    "            logger.warning(\"No backazimuth data available for plot\")\n",
    "        # PLOT 6: Slowness - Row 3, Col 2 (Lower Right)\n",
    "        axslow = fig.add_subplot(gs[2, 1])\n",
    "        \n",
    "        # Add horizontal line for the threshold\n",
    "        axslow.axhline(y=slow_threshold, color='r', linestyle='--', alpha=0.7, \n",
    "                       label=f'Threshold ({slow_threshold})')\n",
    "        \n",
    "        # Get matplotlib times for the slowness data\n",
    "        if ar.select(channel=\"ZGA\") and len(ar.select(channel=\"ZGA\")[0].data) > 0:\n",
    "            slow_times = ar.select(channel=\"ZGA\")[0].times(\"matplotlib\")\n",
    "            \n",
    "            # Check for matching data lengths\n",
    "            if len(slow_times) == len(slow) and len(slow) == len(rel_power):\n",
    "                # Use scatter for slowness, sized by rel_power like backazimuth\n",
    "                scatter_slow = axslow.scatter(slow_times, slow, \n",
    "                            c=rel_power, cmap=global_cmap, s=sizes, alpha=0.7)\n",
    "                \n",
    "                axslow.set_ylabel('Slowness [s/km]', fontsize=11, fontweight='bold')\n",
    "                axslow.set_xlabel('Time (UTC)', fontsize=11, fontweight='bold')\n",
    "                axslow.set_title('Slowness vs. Time', fontsize=12, fontweight='bold', pad=10)\n",
    "                \n",
    "                # Set x-axis ticks every second\n",
    "                axslow.xaxis.set_major_locator(seconds_locator)\n",
    "                axslow.xaxis.set_major_formatter(seconds_formatter)\n",
    "                \n",
    "                # Set time limits to match seismogram\n",
    "                axslow.set_xlim(common_time_limits)\n",
    "                \n",
    "                # Set y-axis limits\n",
    "                axslow.set_ylim(0, max(1.0, np.max(slow)*1.1))\n",
    "                \n",
    "                # Enhanced grid with lines\n",
    "                axslow.grid(True, which='major', axis='both', color='gray', alpha=0.5, linestyle='-')\n",
    "                \n",
    "                axslow.legend(loc='upper right', fontsize=9)\n",
    "                \n",
    "                # Rotate time labels\n",
    "                plt.setp(axslow.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "            else:\n",
    "                logger.warning(f\"Data length mismatch in slowness plot\")\n",
    "        else:\n",
    "            logger.warning(\"No slowness data available for plot\")\n",
    "        \n",
    "        # Colorbar for all plots using the same colormap\n",
    "        if 'pcm' in locals():\n",
    "            cax = fig.add_axes([0.93, 0.3, 0.02, 0.4])  # Position for vertical colorbar\n",
    "            cbar = fig.colorbar(pcm, cax=cax)\n",
    "            cbar.set_label('Relative Power', fontsize=10, fontweight='bold')\n",
    "        \n",
    "        # Save figure\n",
    "        fmt = \"png\"\n",
    "        filename = f'{save_path}UP1_{event_time.strftime(\"%Y%m%d_%H%M%S\")}_array.{fmt}'\n",
    "        \n",
    "\n",
    "        try:\n",
    "            plt.savefig(filename, format=fmt, dpi=300)\n",
    "            logger.info(f\"Saved figure: {filename}\")\n",
    "            successful_plots += 1\n",
    "        except Exception as save_error:\n",
    "            logger.error(f\"Failed to save figure {filename}: {save_error}\")\n",
    "            failed_plots += 1\n",
    "        \n",
    "        plt.close(\"all\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing event at {event_time}: {e}\")\n",
    "        failed_plots += 1\n",
    "        plt.close(\"all\")  # Make sure to close all figures even in case of error\n",
    "\n",
    "logger.info(f\"Processing complete! Successfully created {successful_plots} plots, {failed_plots} failed\")\n",
    "logger.info(f\"Summary: {len(trig_times)} triggers detected, {len(valid_triggers)} written to CSV, {successful_plots} plots created\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
