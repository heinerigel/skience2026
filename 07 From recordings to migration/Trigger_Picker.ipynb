{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dcaf3d00-0c15-4686-8256-f57b8290be45",
   "metadata": {},
   "source": [
    "# How to Get to Your Signals - \"don't be afraid to do simple things\"\n",
    "<img src=\"pictures/UP1_21032024_XG.png\" alt=\"one day recording\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f96b68de-d49f-4ef8-abd5-2517a0672d12",
   "metadata": {},
   "source": [
    "### Application of Simple STA/LTA Triggers \n",
    "<b> Developed by Tobias Megies; modified by J. Wassermann </b>\n",
    "\n",
    "The goals is to see the effect of parameter changes on the selected/triggered waveforms. \n",
    "We start with the import of the relevant python/obspy modules/functions <br>\n",
    "You also my consult the obspy tutorial on how to use and adjust trigger/picker (https://docs.obspy.org/tutorial/code_snippets/trigger_tutorial.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e718951-c2bd-4d70-8e18-ceae48c68bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import optparse\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "#core utilities\n",
    "from obspy.core import UTCDateTime, Stream, AttribDict\n",
    "from obspy import read_inventory\n",
    "#SDS based data input\n",
    "from obspy.clients.filesystem import sds\n",
    "\n",
    "SDS_DATA_PATH = \"/Users/jowa/Skience2026_Data/data_sds/\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746442c6-4fe6-4270-8e0d-a3d09f3573a9",
   "metadata": {},
   "source": [
    "The data set we are working with is from an experiment in 2024 at the lower part of the Grenzgletscher (glacier) \n",
    "located in the Mt. Rosa massif in Switzerland. <br>\n",
    "In next code cell you can change the values of the triggers (see obspy documentation) as well as filter parameter etc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d78eb1-5d68-45c0-8e2d-f953be6db487",
   "metadata": {},
   "source": [
    "<img src=\"pictures/Grenzgletscher_unten.jpg\" alt=\"Grenz24 2500m\" width=\"500\"/> <img src=\"pictures/Lower_MH.jpg\" alt=\"Gorner 2500m\" width=\"500\"/> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f42597b-8641-417e-84ff-b4f867360c77",
   "metadata": {},
   "source": [
    "We start by applying a simple recursive STA/LTA trigger ... but we use the network as a whole <br>\n",
    "So we import the obspy function coincidence_trigger first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf4a395-5a73-49ad-bee5-30f9a98ccfb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#trigger\n",
    "from obspy.signal.trigger import coincidence_trigger\n",
    "par = AttribDict()\n",
    "\n",
    "par.sds_rootdir = SDS_DATA_PATH # link to the data\n",
    "#filter \n",
    "par.filter = AttribDict(type=\"bandpass\", freqmin=5.0, freqmax=30.0,\n",
    "                            corners=2, zerophase=True)\n",
    "#different data sets from two different areas at the glacier\n",
    "trace_ids = {}\n",
    "#trace_ids = {\"XG.UP1..GLZ\": 1, \"XG.UP2..GLZ\": 1,\"XG.UP3..GLZ\": 1,\"XG.UP4..GLZ\": 1,\"XG.UP5..GLZ\": 1,\n",
    "#            \"XG.UP6..GLZ\": 1}\n",
    "trace_ids = {\"XG.BB01..HHZ\": 1, \"XG.ADR1..GLZ\": 1,\"XG.C16..GLZ\": 1,\"XG.C17..GLZ\": 1,\"XG.C29..DLZ\": 1,\n",
    "             \"XG.C2A..DLZ\": 1,\"XG.C68..DLZ\": 1,\"XG.C67..DLZ\": 1,\"XG.S35..GLZ\": 1,\"XG.C65..DLZ\": 1,\"XG.BAS..GLZ\": 1,\\\n",
    "             \"XG.C6A..DLZ\": 1,\"XG.69..DLZ\": 1}\n",
    "coinc_sum = 4.0 # at least for stations must trigger +- simultaneously\n",
    "par.trace_ids = trace_ids\n",
    "\n",
    "# STA/LTA settings - please note: length of sta and lta in seconds \n",
    "par.coincidence = AttribDict(trigger_type=\"recstalta\", sta=1., lta=10,\n",
    "                                 thr_on=2., thr_off=1.9,\n",
    "                                 thr_coincidence_sum=coinc_sum,\n",
    "                                 trace_ids=trace_ids, max_trigger_length=10,\n",
    "                                 trigger_off_extension=1)\n",
    "# output directory\n",
    "par.dir = \"./XG_trigger\" \n",
    "par.logfile = os.path.join(par.dir, \"%s_log.txt\" % \"XG\")\n",
    "par.trigfile = os.path.join(par.dir, \"%s_trigger.txt\" % \"XG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95e610f-bab3-48b6-928e-a0300cc9c7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete the old files to have a fresh start\n",
    "!rm -f \"%s/*.txt\"%par.dir\n",
    "!rm -rf \"%s/*.png\"%par.dir "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df86b10f-1681-40d2-9ea5-afe60a54de39",
   "metadata": {},
   "source": [
    "Now we do the triggering in hourly data chunks ...<br>\n",
    "Pretty long loop ... and sorry for the warnings ... <br>\n",
    "<b> Be carefull about the time intervall ... might be take along time </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1164a728-c1b1-4421-bc23-ba3a625c8211",
   "metadata": {},
   "outputs": [],
   "source": [
    "#select one day of data\n",
    "start = UTCDateTime(2024,3,20)\n",
    "end = UTCDateTime(2024,3,21)\n",
    "t1 = int(UTCDateTime(start).timestamp)\n",
    "t2 = int(UTCDateTime(end).timestamp)\n",
    "times = [UTCDateTime(t) for t in np.arange(t1, t2, 3600)] #split the trigger in 1 hour pieces\n",
    "\n",
    "sds_cl = sds.Client(sds_root=par.sds_rootdir)\n",
    "\n",
    "for time in times:\n",
    "    t1 = time\n",
    "    t2 = t1 + 3600 + 10 #adding extra 10 seconds\n",
    "    st = Stream()\n",
    "    num_stations = 0\n",
    "    station_id = []\n",
    "    possible_coinc_sum = 0\n",
    "    exceptions = []\n",
    "    for trace_id, weight in par.trace_ids.items():\n",
    "        net, sta, loc, cha = trace_id.split(\".\")\n",
    "        station_id.append(\"%s.%s.%s.%s*\"%(net,sta,loc,cha[:-1])) \n",
    "        for comp in \"ZNE\":\n",
    "            cha_ = cha[:-1] + comp\n",
    "            try:\n",
    "               # load in the data channel by channel\n",
    "                tmp = sds_cl.get_waveforms(network=net, station=sta, location=loc, channel=cha_, starttime=t1, endtime=t2)\n",
    "                #read the metadata of the stations\n",
    "                inv = read_inventory(\"./stationxml/station_%s_%s.xml\"%(net,sta))\n",
    "                tmp.attach_response(inv)\n",
    "            except:\n",
    "                exceptions.append(\"%s-%s\" % (sta,cha_))\n",
    "                continue\n",
    "            if comp == cha[-1]:\n",
    "                possible_coinc_sum += weight\n",
    "            st += tmp\n",
    "        num_stations += 1\n",
    "    st.merge(-1)\n",
    "    st.sort()\n",
    "\n",
    "    #preparing the output log files and trigger\n",
    "    trigger = []\n",
    "    summary = []\n",
    "    summary.append(\"#\" * 79)\n",
    "    summary.append(\"######## %s  ---  %s ########\" % (t1, t2))\n",
    "    summary.append(\"#\" * 79)\n",
    "    summary.append(st.__str__(extended=True))\n",
    "    if exceptions:\n",
    "        summary.append(\"#\" * 33 + \" Exceptions  \" + \"#\" * 33)\n",
    "        summary += exceptions\n",
    "    summary.append(\"#\" * 79)\n",
    "\n",
    "    st.traces = [tr for tr in st if tr.stats.npts > 1]\n",
    "\n",
    "    trig = []\n",
    "    mutt = []\n",
    "    if st:\n",
    "        # preprocessing, backup original data for plotting at end\n",
    "        st.detrend(\"linear\")\n",
    "        st.merge(method=1, fill_value=0)\n",
    "        for tr in st:\n",
    "            perc = 1.0 / (tr.stats.endtime - tr.stats.starttime)\n",
    "            perc = min(perc, 1)\n",
    "            tr.taper(type=\"cosine\", max_percentage=perc)\n",
    "            #we live in counts\n",
    "            #tr.remove_sensitivity()\n",
    "\n",
    "        #st.remove_response(water_level=10,output=\"VEL\")\n",
    "        st.sort()\n",
    "        st.trim(t1, t2, pad=True, fill_value=0)\n",
    "        st_trigger = st.copy()\n",
    "        \n",
    "        # filter and trigger\n",
    "        st_trigger.filter(**par.filter)\n",
    "############################### here we finally trigger ##################################\n",
    "        # do the triggering (with additional data at sides to avoid artifacts\n",
    "        trig = coincidence_trigger(stream=st_trigger, details=True,\n",
    "                                  **par.coincidence)\n",
    "        \n",
    "        # restrict trigger list to time span of interest\n",
    "        trig = [t for t in trig if (t1 <= t['time'] <= t2)]\n",
    "\n",
    "############################### now we do the plotting ##################################\n",
    "        for t in trig:\n",
    "            max_similarity = max(list(t['similarity'].values()) + [0])\n",
    "            time_str = str(t['time']).split(\".\")[0]\n",
    "            sta_string = \"-\".join(t['stations'])\n",
    "            info = \"%s %ss %s %.2f %s\"\n",
    "            info = info % (time_str, (\"%.1f\" % t['duration']).rjust(4),\n",
    "                    (\"%i\" % t['cft_peak_wmean']).rjust(3),\n",
    "                    max_similarity, sta_string)\n",
    "            summary.append(info)\n",
    "            sta_string = \",\".join(station_id)\n",
    "            \n",
    "            info = \"%s %s %s %s\"%\\\n",
    "                        (net, str(t['time']), str(t['duration']),str(t['coincidence_sum']))\n",
    "            summary.append(info)\n",
    "            trigger.append(info)\n",
    "            tmp = st_trigger.slice(t['time'] - 1, t['time'] + t['duration'] + 1)\n",
    "            filename = \"%s_%.1f_%i_%s-%s_%.2f_%s.png\"\n",
    "            filename = filename % (time_str, t['duration'],\n",
    "                                   t['cft_peak_wmean'], t['coincidence_sum'],\n",
    "                                   possible_coinc_sum, max_similarity,\n",
    "                                   net)\n",
    "\n",
    "            #now we create figures for later checking of your trigger settings\n",
    "            filename = os.path.join(par.dir, filename)\n",
    "            stations = sorted(set([tr.id.rsplit(\".\", 1)[0] for tr in tmp]))\n",
    "            dpi = 72\n",
    "            fig = plt.figure(figsize=(700.0 / dpi, 400.0 / dpi))\n",
    "            ax = None\n",
    "            for i_, netstaloc in enumerate(stations):\n",
    "                net, sta, loc = netstaloc.split(\".\")\n",
    "                ax = fig.add_subplot(len(stations), 1, i_ + 1, sharex=ax)\n",
    "                for comp, color in zip(\"ENZ\", \"rbk\"):\n",
    "                    tmp_ = tmp.select(network=net,station=sta,location=loc, component=comp).copy()\n",
    "                    tmp_.detrend(\"constant\")\n",
    "                    for tr in tmp_:\n",
    "                        x = tr.times() + (tr.stats.starttime - t['time'])\n",
    "                        ax.plot(x, tr.data, color=color, linewidth=1.2)\n",
    "                ax.text(0.02, 0.95, netstaloc, va=\"top\", ha=\"left\",\n",
    "                        transform=ax.transAxes)\n",
    "            for ax in fig.axes:\n",
    "                ylims = ax.get_ylim()\n",
    "                ax.set_yticks(ylims)\n",
    "                #ax.set_yticklabels([\"%.1e\" % (val * 1) for val in ylims])\n",
    "                ax.set_ylabel(\"a.u.\")\n",
    "            for ax in fig.axes[::2]:\n",
    "                ax.yaxis.set_ticks_position(\"left\")\n",
    "            for ax in fig.axes[1::2]:\n",
    "                ax.yaxis.set_ticks_position(\"right\")\n",
    "            for ax in fig.axes[:-1]:\n",
    "                ax.set_xticks([])\n",
    "            try:\n",
    "                fig.tight_layout()\n",
    "            except:\n",
    "                pass\n",
    "            fig.subplots_adjust(hspace=0)\n",
    "            fig.savefig(filename, dpi=dpi)\n",
    "            plt.close('all')\n",
    "\n",
    "        del tmp\n",
    "        del st_trigger\n",
    "        del tr\n",
    "    del st\n",
    "\n",
    "    summary.append(\"#\" * 79)\n",
    "    summary = \"\\n\".join(summary)\n",
    "    trigger = \"\\n\".join(trigger)\n",
    "    # avoid writing long list of streams when using many event templates\n",
    "    par_tmp = par.copy()\n",
    "    summary += \"\\n\" + \"\\n\".join((\"%s=%s\" % (k, v) for k, v in par_tmp.items()))\n",
    "    with open(par.logfile, \"at\") as fh:\n",
    "        fh.write(summary + \"\\n\")\n",
    "    with open(par.trigfile, \"at\") as fu:\n",
    "        fu.write(trigger + \"\\n\")\n",
    "    del summary\n",
    "    del trigger\n",
    "    del mutt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d93415-8b93-418a-bb0b-ee8f2926d3ae",
   "metadata": {},
   "source": [
    "So let's have a look ....\n",
    "Well still a lot of false triggers (of course depending what you expect)\n",
    "Can we do better?\n",
    "### Of Course if we use"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dabc7ee-2014-4748-98a6-9e9bf2192758",
   "metadata": {},
   "source": [
    "# Application of an AR-Picker Schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83242fe0-6c04-4a49-83aa-9b4a9359c4e7",
   "metadata": {},
   "source": [
    "Pretty sofisticated picker which is using ar-prediction in combination with forward-backward STA/LTA. If tuned a very good picker also for S-waves....\n",
    "### But ... the tuning is tedious "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a34acd-9fb7-4fc4-ac6a-55981f39cd68",
   "metadata": {},
   "source": [
    "Again lets load the needed libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535208f5-1ad8-444b-92a7-ebdd87a0405f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from obspy.core.event import Catalog, Event, Origin, Magnitude,  Arrival, Pick, WaveformStreamID\n",
    "from obspy.core import UTCDateTime, Stream, AttribDict\n",
    "from obspy import read_inventory\n",
    "#from obspy.clients.fdsn import Client\n",
    "from obspy.clients.filesystem import sds\n",
    "#import operator\n",
    "#import matplotlib\n",
    "\n",
    "#from matplotlib.backends.backend_pdf import PdfPages\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import numpy as np\n",
    "#from matplotlib.transforms import offset_copy\n",
    "\n",
    "import os\n",
    "#import optparse\n",
    "import warnings\n",
    "from copy import deepcopy\n",
    "from configparser import ConfigParser, NoOptionError, NoSectionError\n",
    "\n",
    "from obspy.signal.trigger import ar_pick\n",
    "\n",
    "#just ment as an example how to use external cfg files .... but here for simplicity we set the ar parameter right in the code\n",
    "#config_file = \"./ar_picker.cfg\"\n",
    "#config = ConfigParser(allow_no_value=True)\n",
    "# make all config keys case sensitive\n",
    "#config.optionxform = str\n",
    "#config.read(config_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80f6ac0-6b57-469c-ac3d-0d79a39ed620",
   "metadata": {},
   "source": [
    "For further insights of the functionality please refer to the ObsPy documentation (http://docs.obspy.org)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e674113-c4f1-455c-b112-915ac1f5450b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _arpicker(stream):\n",
    "    \"\"\"\n",
    "    Run AR picker on all streams and set P/S picks accordingly.\n",
    "    Also displays a message.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        f1 = 5.0 #config.getfloat(\"ar_picker\", \"f1\")\n",
    "        f2 = 50.0 #config.getfloat(\"ar_picker\", \"f2\")\n",
    "        sta_p = 0.05 #config.getfloat(\"ar_picker\", \"sta_p\")\n",
    "        lta_p = 0.5 #0.5 #config.getfloat(\"ar_picker\", \"lta_p\")\n",
    "        sta_s = 0.05 #config.getfloat(\"ar_picker\", \"sta_s\")\n",
    "        lta_s = 0.5 #0.5 #config.getfloat(\"ar_picker\", \"lta_s\")\n",
    "        m_p = 2 #config.getint(\"ar_picker\", \"m_p\")\n",
    "        m_s = 4 #config.getint(\"ar_picker\", \"m_s\")\n",
    "        l_p = 0.1 #config.getfloat(\"ar_picker\", \"l_p\")\n",
    "        l_s = 0.1 #config.getfloat(\"ar_picker\", \"l_s\")\n",
    "    except (NoOptionError, NoSectionError) as e:\n",
    "        msg = ('To use AR Picker, you need to have to set the following'\n",
    "               'keys set: \"f1\", '\n",
    "               '\"f2\", \"lta_p\", \"sta_p\", \"lta_s\", \"sta_s\", \"m_p\", \"m_s\", '\n",
    "               '\"l_p\", \"l_s\" (compare documentation for '\n",
    "               'obspy.signal.trigger.ar_pick\\n%s') % str(e)\n",
    "        self.error(msg)\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        z = stream.select(component=\"Z\")[0]\n",
    "        n = stream.select(component=\"N\")[0]\n",
    "        e = stream.select(component=\"E\")[0]\n",
    "    except IndexError:\n",
    "        msg = ('AR picker currently only implemented for Z/N/E data, '\n",
    "               'but provided stream was:\\n%s') % stream\n",
    "        print(msg)\n",
    "\n",
    "    try:\n",
    "        assert z.stats.sampling_rate == n.stats.sampling_rate == \\\n",
    "            e.stats.sampling_rate\n",
    "    except AssertionError:\n",
    "        msg = ('AR picker needs same sampling rate on all traces '\n",
    "               'but provided stream was:\\n%s') % st\n",
    "        print(msg)\n",
    "    spr = z.stats.sampling_rate\n",
    "    p, s = ar_pick(z.data, n.data, e.data, spr, f1, f2, lta_p, sta_p,\n",
    "                   lta_s, sta_s, m_p, m_s, l_p, l_s)\n",
    "    picks = []\n",
    "    #we write the phases as obspy pick struct for possible futher use in location codes\n",
    "    for t, phase_hint, tr in zip((p, s), 'PS', (z, n)):\n",
    "        if t > 0:\n",
    "            pick = Pick(time=z.stats.starttime.timestamp+t,\n",
    "                                     waveform_id=WaveformStreamID(network_code=z.stats.network,\n",
    "                                     station_code=z.stats.station),\n",
    "                                     phase_hint=phase_hint)\n",
    "            if phase_hint == \"S\" and (t - (picks[0].time.timestamp - z.stats.starttime.timestamp)) > 0 \\\n",
    "                               and (t - (picks[0].time.timestamp - z.stats.starttime.timestamp)) < 0.5:\n",
    "                picks.append(pick)\n",
    "            elif phase_hint == \"P\":\n",
    "                picks.append(pick)\n",
    "    return (picks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b12ae49-16b9-453b-be74-8a4b9f6cfc98",
   "metadata": {},
   "source": [
    "Same settings and data sets as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c76917-44f3-4dc1-8c8c-8449505bee16",
   "metadata": {},
   "outputs": [],
   "source": [
    "par = AttribDict()\n",
    "par.sds_rootdir = SDS_DATA_PATH\n",
    "\n",
    "par.filter = AttribDict(type=\"bandpass\", freqmin=5.0, freqmax=30.0,\n",
    "                            corners=2, zerophase=True)\n",
    "trace_ids = {}\n",
    "\n",
    "trace_ids = {\"XG.BB01..HHZ\": 1, \"XG.ADR1..GLZ\": 1,\"XG.C16..GLZ\": 1,\"XG.C17..GLZ\": 1,\"XG.C29..DLZ\": 1,\n",
    "             \"XG.C2A..DLZ\": 1,\"XG.C68..DLZ\": 1,\"XG.C67..DLZ\": 1,\"XG.C65..DLZ\": 1,\n",
    "             \"XG.C6A..DLZ\": 1,\"XG.C69..DLZ\": 1}\n",
    "par.trace_ids = trace_ids\n",
    "\n",
    "#Here we restrict our picking for events with more than X triggered stations\n",
    "par.threshold_coin = 7\n",
    "\n",
    "par.dir = \"./XG_trigger\" \n",
    "par.logfile = os.path.join(par.dir, \"%s_log.txt\" % \"XG\")\n",
    "par.trigfile = os.path.join(par.dir, \"%s_trigger.txt\" % \"XG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469c2fe5-7af2-4b70-9427-4e7a875b7f85",
   "metadata": {},
   "source": [
    "Let's use the STA/LTA triggers as first guess and try to get better picks for seleceted waveforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d69b14-83b8-4e6e-80a5-053ea446a6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm \"%s/AR-*.png\"%(par.dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d027b118-4878-473a-a4b1-06dc9d6fd036",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "trig_data = np.genfromtxt(\"%s/XG_trigger.txt\"%par.dir,usecols=(1,2,3),dtype='str',delimiter=\" \")\n",
    "\n",
    "\n",
    "def getdata(t1):\n",
    "    num_stations = 0\n",
    "    station_id = []\n",
    "    exceptions = []\n",
    "    st= Stream()\n",
    "    for trace_id,_ in par.trace_ids.items():\n",
    "        net, sta, loc, cha = trace_id.split(\".\")\n",
    "        station_id.append(\"%s.%s.%s.%s*\"%(net,sta,loc,cha[:-1]))\n",
    "        \n",
    "        for comp in \"ZNE\":\n",
    "            cha_ = cha[:-1] + comp\n",
    "            try:\n",
    "                tmp = sds_cl.get_waveforms(network=net, station=sta, location=loc, channel=cha_, starttime=t1, endtime=t1+3600)\n",
    "                inv = read_inventory(\"./stationxml/station_%s_%s.xml\"%(net,sta))\n",
    "                tmp.attach_response(inv)\n",
    "                st += tmp\n",
    "                num_stations += 1\n",
    "            except:\n",
    "                exceptions.append(\"%s-%s\" % (sta,cha_))\n",
    "                continue\n",
    "    if len(exceptions)>0:\n",
    "        print(exceptions)\n",
    "    st.merge(-1)\n",
    "    st.sort()\n",
    "    return st,station_id,num_stations\n",
    "\n",
    "trigger_times = []\n",
    "duration = []\n",
    "coincidense = []\n",
    "allpicks = []\n",
    "sds_cl = sds.Client(sds_root=par.sds_rootdir)\n",
    "t_0 = UTCDateTime(trig_data[0][0])\n",
    "t_1 = UTCDateTime(t_0.year,t_0.month,t_0.day,t_0.hour) \n",
    "\n",
    "st,stations,no_s = getdata(t_1)\n",
    "\n",
    "## We restrict ourself to 50 possible events ....\n",
    "\n",
    "for i in range(0,50): #len(trig_data)):\n",
    "    t_t = (UTCDateTime(trig_data[i][0]))\n",
    "    duration = float(trig_data[i][1])\n",
    "    coin = float(trig_data[i][2])\n",
    "    if coin > par.threshold_coin:\n",
    "        if UTCDateTime(t_t.year,t_t.month,t_t.day,t_t.hour) > t_1:\n",
    "            #get a new junk of data\n",
    "            st,stations,no_s = getdata(UTCDateTime(t_t.year,t_t.month,t_t.day,t_t.hour))\n",
    "\n",
    "        #prepare plotting as well\n",
    "        dpi = 72\n",
    "        fig = plt.figure(figsize=(700.0 / dpi, 400.0 / dpi))\n",
    "        ax = None\n",
    "        stt = st.copy()\n",
    "        stt.detrend(\"linear\")\n",
    "        \n",
    "        stt.filter(**par.filter)\n",
    "        stt.trim(t_t-1,t_t+duration+1)\n",
    "        stt.taper(0.01,type=\"cosine\")\n",
    "\n",
    "        for i_,trace_id in enumerate(stations):\n",
    "            net, sta, loc, cha = trace_id.split(\".\")\n",
    "            sstt = stt.select(station=sta)\n",
    "            ax = fig.add_subplot(len(stations), 1, i_ + 1, sharex=ax)\n",
    "            try:\n",
    "                picks = _arpicker(sstt)\n",
    "            except:\n",
    "                print(\"did not work for station: \",sta)\n",
    "            stat = {}\n",
    "            for p in picks:\n",
    "                if p.phase_hint == 'P':\n",
    "                    stat.update({\"network\": net})\n",
    "                    stat.update({\"station\": sta})\n",
    "                    stat.update({\"P\": p.time})\n",
    "                    stat.update({\"channel\": cha[:-1]+\"Z\"})\n",
    "                if p.phase_hint == 'S':\n",
    "                    stat.update({\"S\": p.time})\n",
    "            if stat[\"P\"] or stat[\"s\"]:\n",
    "                tr = sstt.select(station=sta,component=\"Z\")[0]\n",
    "                x = tr.times(type=\"matplotlib\") \n",
    "                x = tr.times(type=\"matplotlib\") - tr.stats.starttime.matplotlib_date\n",
    "                ax.plot(x, tr.data, color=\"k\", linewidth=1.2)\n",
    "                ax.text(0.02, 0.95, sta, va=\"top\", ha=\"left\",\n",
    "                        transform=ax.transAxes) \n",
    "                try:\n",
    "                    ax.axvline(x=stat[\"P\"].matplotlib_date - tr.stats.starttime.matplotlib_date,color=\"r\")\n",
    "                except:\n",
    "                    pass\n",
    "                try:\n",
    "                    ax.axvline(x=stat[\"S\"].matplotlib_date- tr.stats.starttime.matplotlib_date,color=\"b\")\n",
    "                except:\n",
    "                    pass\n",
    "        \n",
    "        fig.subplots_adjust(hspace=0)            \n",
    "        plt.savefig('%s/AR-%04d%02d%02dT%02d-%d.png'%(par.dir,t_t.year,t_t.month,t_t.day,t_t.hour,i),dpi=300)\n",
    "        plt.close('all')\n",
    "            \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
